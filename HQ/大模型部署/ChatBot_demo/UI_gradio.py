import gradio as gr
import requests

# FastAPI åç«¯åœ°å€
FASTAPI_URL = "http://localhost:7000/chat"


def chat_with_backend(prompt, history, sys_prompt, history_len, temperature, top_p, max_tokens, stream):
    # history: ["role": "user", metadata: {'title':None}, "content": "xxx"]
    # å»æ‰ metadata å­—æ®µ
    history_new = [{"role": item["role"], "content": item["content"]} for item in history]

    # æ„å»ºè¯·æ±‚æ•°æ®
    data = {
        "query": prompt,
        "sys_prompt": sys_prompt,
        "history": history_new,
        "history_len": history_len,
        "temperature": temperature,
        "top_p": top_p,
        "max_tokens": max_tokens
    }

    # å‘é€è¯·æ±‚åˆ° FastAPI åç«¯
    response = requests.post(FASTAPI_URL, json=data, stream=True)

    if response.status_code == 200:
        full_response = ""
        if stream:
            for chunk in response.iter_content(chunk_size=None, decode_unicode=True):
                full_response += chunk
                yield full_response
        else:
            for chunk in response.iter_content(chunk_size=None, decode_unicode=True):
                full_response += chunk
            yield full_response


# ä½¿ç”¨ gr.Blocks åˆ›å»ºä¸€ä¸ªå—ï¼ˆæ„å»ºç•Œé¢ï¼‰ï¼Œè®¾ç½®å¯ä»¥å¡«å……å®½é«˜
with gr.Blocks(fill_width=True, fill_height=True) as demo:
    # åˆ›å»ºä¸€ä¸ªæ ‡ç­¾é¡µ
    with gr.Tab("ğŸ¦„ChatBot"):
        # æ·»åŠ æ ‡é¢˜
        gr.Markdown("## ğŸ¦„ ä»Šå¤©å‡†å¤‡åšä»€ä¹ˆï¼Ÿ")

        # åˆ›å»ºä¸€ä¸ªè¡Œå¸ƒå±€
        with gr.Row():
            # åˆ›å»ºä¸€ä¸ªå·¦ä¾§çš„åˆ—å¸ƒå±€: è®¾ç½® AI çš„å‚æ•°ï¼ˆæ¯”ä¾‹ä¸º1ï¼‰
            with gr.Column(scale=1, variant="panel") as left_col:
                sys_prompt = gr.Textbox(label="ç³»ç»Ÿæç¤ºè¯", value="ä½ æ˜¯ä¸€ä¸ªçˆ±ä½¿ç”¨é¢œæ–‡å­—çš„ç§äººåŠ©æ‰‹")
                history_len = gr.Slider(label="å†å²å¯¹è¯é•¿åº¦", minimum=0, maximum=10, value=5, step=1)
                temperature = gr.Slider(label="æ¸©åº¦", minimum=0.01, maximum=2.0, value=0.7, step=0.01)
                top_p = gr.Slider(label="LLMé‡‡æ ·æ¦‚ç‡", minimum=0.01, maximum=1.0, value=0.7, step=0.01)
                max_tokens = gr.Slider(label="æœ€å¤§tokenæ•°", minimum=256, maximum=4096, value=1024, step=8)
                stream = gr.Checkbox(label="æµå¼å“åº”", value=True)

            # åˆ›å»ºä¸€ä¸ªåˆ—å¸ƒå±€: æ˜¾ç¤ºç”¨æˆ·è¾“å…¥å’ŒAIçš„å›ç­”ï¼ˆæ¯”ä¾‹ä¸º10ï¼‰
            with gr.Column(scale=10) as main_col:
                # åˆ›å»ºä¸€ä¸ªChatbotç»„ä»¶ï¼ˆèŠå¤©ç•Œé¢ï¼‰ï¼Œé«˜åº¦ä¸º500px
                chatbot = gr.Chatbot(type="messages", height=500)
                # åˆ›å»ºChatInterfaceï¼Œç”¨äºå¤„ç†èŠå¤©çš„é€»è¾‘
                gr.ChatInterface(
                    fn=chat_with_backend,
                    chatbot=chatbot,
                    additional_inputs=[sys_prompt, history_len, temperature, top_p, max_tokens, stream],
                    type="messages"
                )

# è¿è¡Œ Gradio åº”ç”¨
if __name__ == "__main__":
    demo.launch(server_port=7860, inbrowser=True)