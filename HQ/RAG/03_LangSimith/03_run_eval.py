from dotenv import load_dotenv
import os
from langsmith import evaluate
from langsmith.schemas import Run, Example
from langsmith.evaluation import LangChainStringEvaluator
from langchain_openai import ChatOpenAI

# ----------------------------------------
# é…ç½®ç¯å¢ƒå˜é‡
# ----------------------------------------
load_dotenv('../RAG.env')
openai_api_key = os.getenv('OPENAI_API_KEY')
openai_url = os.getenv('MODEL_URL')
langsmith_api_key = os.getenv('LANGSMITH_API_KEY')
os.environ["LANGCHAIN_TRACING_V2"] = "false"
os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"

# è£åˆ¤æ¨¡å‹
judge_llm = ChatOpenAI(
    model="glm-4.5-flash",
    openai_api_key=openai_api_key,
    base_url=openai_url,
    temperature=0
)

# ----------------------------------------
# å®šä¹‰ target_function
# LangSmith ä¼šå‘å®ƒä¼  {"Question": "..."} è¿™æ ·çš„å­—å…¸
# ----------------------------------------
# å®šä¹‰ç®€å•çš„rag
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
prompt = ChatPromptTemplate.from_template("è¯·å›ç­”ï¼š{question}")
rag_chain = prompt | judge_llm | StrOutputParser()

# å®šä¹‰ target_function
def target_fn(example_inputs: dict):
    q = example_inputs["Question"]
    result = rag_chain.invoke(q)
    # getattr(å¯¹è±¡, è¦è·å–çš„å±æ€§å, é»˜è®¤å€¼)
    answer_text = getattr(result, "content", str(result))
    return {"output": answer_text}


# ----------------------------------------
# Evaluator #1ï¼šè§„åˆ™è¯„ä¼°
# ç±»ä¼¼å‘é‡ç›¸ä¼¼åº¦ï¼Œéƒ½æ˜¯å†™ä»£ç ï¼Œé€šè¿‡ä»£ç é€»è¾‘åˆ¤æ–­
# ----------------------------------------
def concise_evaluator(root_run: Run, example: Example):
    """
        root_run.outputs["output"]   : æ¨¡å‹è¾“å‡º
        example.outputs["Answer"]    : å‚è€ƒç­”æ¡ˆ
    """
    stu_answer = root_run.outputs.get("output", "")
    true_answer = example.outputs.get("Answer", "")

    # ç®€å•è§„åˆ™ï¼šç”Ÿæˆç­”æ¡ˆé•¿åº¦ < å‚è€ƒç­”æ¡ˆä¸¤å€ â†’ è®¤ä¸ºç®€æ´
    score = 1 if len(stu_answer) < len(true_answer) * 2 else 0

    return {
        "key": "is_concise",
        "score": score
    }


# ----------------------------------------
# Evaluator #2ï¼šä½¿ç”¨ LangChain å†…ç½®çš„ QA è¯„ä¼°å™¨ï¼ˆStringEvaluatorï¼‰
# å®ƒä¼šè‡ªåŠ¨åˆ¤æ–­ â€œæ¨¡å‹å›ç­”æ˜¯å¦ç¬¦åˆå‚è€ƒç­”æ¡ˆâ€
# ----------------------------------------
qa_evaluator = LangChainStringEvaluator(
    evaluator="qa",
    config={"llm": judge_llm},
    prepare_data = lambda run, example: {
        "prediction": run.outputs.get("output"),  # å¯¹åº” RAG çš„è¾“å‡º
        "reference": example.outputs.get("Answer"),  # å¯¹åº”æ•°æ®é›†çš„ç­”æ¡ˆ
        "input": example.inputs.get("Question"),     # å¯¹åº”æ•°æ®é›†çš„é—®é¢˜
    }
)


# ----------------------------------------
# Evaluator #3ï¼šè‡ªå®šä¹‰ LLM-as-a-Judgeï¼ˆå¯å†™æç¤ºè¯ï¼‰
# ----------------------------------------
JUDGE_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„è¯„ä¼°åŠ©ç†ï¼Œè¯·ç”¨ 0 æˆ– 1 åˆ¤æ–­æ¨¡å‹å›ç­”æ˜¯å¦æ­£ç¡®ã€‚

é—®é¢˜ï¼š{question}
å‚è€ƒç­”æ¡ˆï¼š{reference}
æ¨¡å‹å›ç­”ï¼š{output}

å¦‚æœæ¨¡å‹ç­”æ¡ˆè¯­ä¹‰ä¸Šæ­£ç¡® â†’ 1
å¦åˆ™ â†’ 0

è¯·ç›´æ¥è¿”å›ä¸€ä¸ªæ•°å­—ï¼ˆ0 æˆ– 1ï¼‰ï¼Œä¸è¦è§£é‡Šã€‚
"""

def llm_judge_evaluator(root_run: Run, example: Example):
    q = example.inputs["Question"]
    true_answer = example.outputs["Answer"]
    stu_answer = root_run.outputs["output"]

    _prompt = JUDGE_PROMPT.format(
        question=q,
        reference=true_answer,
        output=stu_answer
    )
    # è°ƒç”¨å¤§æ¨¡å‹æ‰“åˆ†
    result = judge_llm.invoke(_prompt)

    try:
        score = int(result.content.strip()[0])
    except ValueError:
        score = 0

    return {"key": "judge_correctness", "score": score}


# ----------------------------------------
# ä¸»å…¥å£ï¼šè°ƒç”¨ evaluate()
# ----------------------------------------
if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹è¯„ä¼°ï¼Œè¯·ç¨ç­‰â€¦â€¦")


    evaluate(
        target_fn,                            # ç›®å‰ç‰ˆæœ¬å¿…é¡»ç”¨ä½ç½®å‚æ•°
        data="example-code",                  # åœ¨ LangSmith åˆ›å»ºçš„æ•°æ®é›†åç§°
        evaluators=[
            concise_evaluator,                # è§„åˆ™ evaluator
            qa_evaluator.as_run_evaluator(),  # å†…ç½® QA è¯„ä¼°å™¨
            llm_judge_evaluator               # LLM-as-a-Judge
        ],
        experiment_prefix="rag-eval-demo",    # å®éªŒå‰ç¼€ï¼Œä¼šåœ¨ LangSmith é‡Œçœ‹åˆ°
        max_concurrency = 1                   # æœ€å¤§å¹¶å‘æ•°
    )

    print("ğŸ‰ å®Œæˆï¼è¯·æ‰“å¼€ LangSmith dashboard æŸ¥çœ‹å¯è§†åŒ–ç»“æœï¼")